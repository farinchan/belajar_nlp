{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat Tokenizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "\n",
    "Di sini, kita membuat sebuah objek Tokenizer dengan parameter num_words=100. Ini berarti bahwa Tokenizer akan mempertimbangkan hingga 100 kata paling umum saat memproses teks. Artinya, jika teks yang kamu punya memiliki lebih dari 100 kata unik, hanya 100 kata yang paling sering muncul yang akan diperhatikan.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menganalisis Teks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "Pada langkah ini, Tokenizer dilatih (atau fit) pada sekumpulan kalimat (yang disimpan dalam variabel sentences). Selama proses ini, Tokenizer akan membuat daftar kata yang muncul di kalimat-kalimat tersebut dan menghitung seberapa sering masing-masing kata muncul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kamus Kata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(word_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "Setelah Tokenizer dilatih, kita dapat mengakses word_index, yaitu sebuah kamus (dictionary) yang berisi pasangan kata dan indeksnya. Indeks ini menunjukkan urutan kata berdasarkan frekuensinya. Kata yang paling sering muncul akan memiliki indeks 1, kata yang berikutnya paling sering muncul akan memiliki indeks 2, dan seterusnya.\n",
    "\n",
    "\n",
    "print(word_index)\n",
    "\n",
    "Terakhir, kita mencetak isi dari word_index ke layar. Ini akan menampilkan daftar kata yang ditemukan dalam kalimat-kalimat tadi, beserta indeks masing-masing kata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
